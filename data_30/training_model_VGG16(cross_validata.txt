training_model_VGG16(cross_validata_model)
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torch.optim as optim
from sklearn.metrics import f1_score, confusion_matrix
import logging
from sklearn.model_selection import KFold
from torch.utils.data import Subset

# 設定 logging
logging.basicConfig(
    filename='/home/u3861345/preprocess_and_model_training/log_data/training_model_VGG16.log',   # 設置日誌文件路徑
    level=logging.INFO,                          # 設置日誌級別
    format='%(asctime)s - %(levelname)s - %(message)s',  # 設置日誌格式
    datefmt='%Y-%m-%d %H:%M:%S'
)

class VGG16_3D(nn.Module):
    def __init__(self):
        super(VGG16_3D, self).__init__()
        
        self.features = nn.Sequential(
            # Block 1
            nn.Conv3d(1, 64, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv3d(64, 64, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool3d(kernel_size=2, stride=2),
            
            # Block 2
            nn.Conv3d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv3d(128, 128, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool3d(kernel_size=2, stride=2),
            
            # Block 3
            nn.Conv3d(128, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv3d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv3d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool3d(kernel_size=2, stride=2),
            
            # Block 4
            nn.Conv3d(256, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv3d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv3d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool3d(kernel_size=2, stride=2),
            
            # Block 5
            nn.Conv3d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv3d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv3d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool3d(kernel_size=2, stride=2),
        )
        
        # 經過5次池化後，輸出特徵圖的大小為 4x4x2，有512個通道
        self.classifier = nn.Sequential(
            nn.Linear(512 * 16 * 16 * 4, 4096), # 這是512,512,128也就是說是原本的
            # nn.Linear(512 * 4 * 4 * 4, 4096),   # 這是128,128,128也就是說是resize的
            
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Linear(4096, 1),  # 二分類問題的輸出層
        )
    
    def forward(self, x):
        x = self.features(x)
        x = torch.flatten(x, 1)
        x = self.classifier(x)
        return x

def train_model(train_loader, model, criterion, optimizer, num_epochs):
    train_accuracies = []

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        for batch in train_loader:
            inputs = batch["vol"]
            labels = batch["label"].clone().detach().float()

            optimizer.zero_grad()
            outputs = model(inputs)
            
            outputs = outputs.view(-1)
            labels = labels.view(-1)
            
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item()
            
            predicted = (outputs >= 0.5).float()
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
        
        epoch_loss = running_loss / len(train_loader)
        train_accuracy = correct / total
        train_accuracies.append(train_accuracy)
        
        logging.info(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {train_accuracy:.4f}')

def test_model(test_loader, model, criterion):
    model.eval()
    total_test_loss = 0.0
    correct_test = 0
    total_test = 0

    all_labels = []
    all_predictions = []
    
    # 用於計算靈敏度和特異性
    TP = 0
    TN = 0
    FP = 0
    FN = 0

    with torch.no_grad():
        for batch in test_loader:
            inputs = batch["vol"]
            labels = batch["label"].clone().detach().float()

            # 使用 logging 來記錄測試標籤
            logging.info(f'Test Labels: {labels}, Type: {labels.dtype}, Unique Values: {labels.unique()}')

            outputs = model(inputs)
            outputs = outputs.view(-1)

            # 使用 logging 來記錄模型輸出
            logging.info(f'Raw outputs: {outputs}, Type: {outputs.dtype}')

            labels = labels.view(-1)
            
            loss = criterion(outputs, labels)
            total_test_loss += loss.item()
            
            predicted = (outputs >= 0.5).float()
            total_test += labels.size(0)
            correct_test += (predicted == labels).sum().item()
            
            # 收集所有標籤和預測值，後續計算 F1 Score 和 confusion matrix
            all_labels.extend(labels.cpu().numpy())
            all_predictions.extend(predicted.cpu().numpy())

        # 計算 F1 Score 和混淆矩陣
        f1 = f1_score(all_labels, all_predictions)
        cm = confusion_matrix(all_labels, all_predictions)
        
        # 混淆矩陣中的 TP, TN, FP, FN
        TN, FP, FN, TP = cm.ravel()
        
        average_test_loss = total_test_loss / len(test_loader)
        test_accuracy = correct_test / total_test
        
        # 計算靈敏度和特異性
        sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0
        specificity = TN / (TN + FP) if (TN + FP) > 0 else 0
        
        # 使用 logging 來記錄測試結果
        logging.info(f'Average Test Loss: {average_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')
        logging.info(f'Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}')
        logging.info(f'F1 Score: {f1:.4f}')
        logging.info(f'Confusion Matrix:\n{cm}')

    return test_accuracy  # 返回準確率

def cross_validate_model(train_data, criterion, num_epochs, k):
    kf = KFold(n_splits=k, shuffle=True, random_state=42)
    fold_accuracies = []

    best_model = None
    best_accuracy = 0.0


    for fold, (train_idx, test_idx) in enumerate(kf.split(train_data)):
        logging.info(f'Fold {fold+1}/{k}')
        
        train_subset = Subset(train_data, train_idx)
        test_subset = Subset(train_data, test_idx)  # K-fold 交叉驗證中的驗證集
        
        train_loader = DataLoader(train_subset, batch_size=8)
        test_loader = DataLoader(test_subset, batch_size=8)
        
        model = VGG16_3D() 

        optimizer = optim.Adam(model.parameters(), lr=1e-6, weight_decay=1e-5)
        
        train_model(train_loader, model, criterion, optimizer, num_epochs)
        fold_accuracy = test_model(test_loader, model, criterion)  # K-fold 交叉驗證的結果

        fold_accuracies.append(fold_accuracy)
        logging.info(f'Fold {fold+1} Accuracy: {fold_accuracy:.4f}')

        # 保存最佳模型
        if fold_accuracy > best_accuracy:
            best_accuracy = fold_accuracy
            best_model = model

    logging.info(f'Average K-Fold Accuracy: {sum(fold_accuracies) / len(fold_accuracies):.4f}')
    
    return best_model

if __name__ == "__main__":
    transformed_train_data = torch.load('/home/u3861345/preprocess_and_model_training/PT/train_first_try.pt')
    transformed_test_data = torch.load('/home/u3861345/preprocess_and_model_training/PT/test_first_try.pt')

    # transformed_train_data = torch.load('/home/u3861345/preprocess_and_model_training/PT/train_resize_mask_and_patient.pt')
    # transformed_test_data = torch.load('/home/u3861345/preprocess_and_model_training/PT/test_resize_mask_and_patient.pt')
    
    criterion = nn.BCEWithLogitsLoss()  # 損失函數 用於二元分類問題
    
    # 傳入訓練資料和驗證資料
    final_model = cross_validate_model(transformed_train_data, criterion, num_epochs=50, k=8)

    # 使用外部測試資料進行最終測試
    test_loader = DataLoader(transformed_test_data, batch_size=1)
    logging.info("Final Test on External Test Set:")
    test_model(test_loader, final_model, criterion)