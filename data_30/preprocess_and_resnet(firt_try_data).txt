import os
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
import logging
from glob import glob
from monai.transforms import (
    Compose,
    EnsureChannelFirstD,
    LoadImaged,
    Resized,
    ToTensord,
    Spacingd,
    ScaleIntensityRanged,
    RandRotateD,         # 使用隨機旋轉
    RandAffineD,         # 使用隨機仿射變換
    RandZoomD,           # 使用隨機縮放
    RandGaussianNoiseD,  # 使用高斯雜訊
)
from monai.utils import set_determinism
from monai.data import Dataset

# 設定 logging
logging.basicConfig(
    filename='/home/u3861345/preprocess_and_model_training/log_data/training_model_resnet.log',   # 設置日誌文件路徑
    level=logging.INFO,                          # 設置日誌級別
    format='%(asctime)s - %(levelname)s - %(message)s',  # 設置日誌格式
    datefmt='%Y-%m-%d %H:%M:%S'
)

def prepare(in_dir, label_file, pixdim=(1.0, 1.0, 1.0), a_min=-200, a_max=200, spatial_size=[128, 128, 128]):
    set_determinism(seed=0)

    labels_df = pd.read_csv(label_file)
    
    # 獲取訓練和測試影像的路徑
    path_train_volumes = sorted(glob(os.path.join(in_dir, "TrainVolumes", "*.nii.gz")))
    path_test_volumes = sorted(glob(os.path.join(in_dir, "TestVolumes", "*.nii.gz")))

    # 構建訓練資料列表，包含影像路徑和對應的標籤
    train_files = []
    for image_name in path_train_volumes:
        patient_id = os.path.basename(image_name).replace('.nii.gz', '')
        label_value = labels_df.loc[labels_df['patient_id'] == patient_id, 'label'].values[0]
        train_files.append({"vol": image_name, "label": label_value})

    # 構建測試資料列表
    test_files = []
    for image_name in path_test_volumes:
        patient_id = os.path.basename(image_name).replace('.nii.gz', '')
        label_value = labels_df.loc[labels_df['patient_id'] == patient_id, 'label'].values[0]
        test_files.append({"vol": image_name, "label": label_value})
    
    # 定義訓練資料的轉換，包括資料增強
    train_transforms = Compose(
        [
            LoadImaged(keys=["vol"]),
            EnsureChannelFirstD(keys=["vol"]),
            Spacingd(keys=["vol"], pixdim=pixdim, mode="bilinear"),
            ScaleIntensityRanged(
                keys=["vol"], a_min=a_min, a_max=a_max,
                b_min=0.0, b_max=1.0, clip=True
            ),
            Resized(keys=["vol"], spatial_size=spatial_size),
            
            # 資料增強操作
            RandRotateD(
                keys=["vol"], range_x=np.pi/12, prob=0.5
            ),  # 隨機旋轉，角度範圍調整為弧度制
            RandAffineD(
                keys=["vol"], prob=0.5,
                rotate_range=(np.pi/18, np.pi/18, np.pi/18),
                translate_range=(10, 10, 10),
                scale_range=(0.9, 1.1)
            ),  # 隨機仿射變換
            RandZoomD(
                keys=["vol"], min_zoom=0.9, max_zoom=1.1, prob=0.5
            ),  # 隨機縮放
            RandGaussianNoiseD(
                keys=["vol"], mean=0.0, std=0.1, prob=0.5
            ),  # 高斯雜訊
            
            ToTensord(keys=["vol"]),
        ]
    )

    # 定義測試資料的轉換
    test_transforms = Compose(
        [
            LoadImaged(keys=["vol"]),
            EnsureChannelFirstD(keys=["vol"]),
            Spacingd(keys=["vol"], pixdim=pixdim, mode="bilinear"),
            ScaleIntensityRanged(
                keys=["vol"], a_min=a_min, a_max=a_max,
                b_min=0.0, b_max=1.0, clip=True
            ),
            Resized(keys=["vol"], spatial_size=spatial_size),
            ToTensord(keys=["vol"]),
        ]
    )

    # 創建訓練和測試資料集
    train_ds = Dataset(data=train_files, transform=train_transforms)
    test_ds = Dataset(data=test_files, transform=test_transforms)

    return train_ds, test_ds

# 定義模型
class BasicBlock3D(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super(BasicBlock3D, self).__init__()
        # 3D 卷積層 1
        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm3d(out_channels)
        # 3D 卷積層 2
        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm3d(out_channels)
        
        # 如果輸入與輸出通道不匹配，則使用 1x1 卷積進行降維
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv3d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm3d(out_channels)
            )

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out

class ResNet3D(nn.Module):
    def __init__(self, num_classes):
        super(ResNet3D, self).__init__()
        self.conv1 = nn.Conv3d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm3d(64)
        self.pool1 = nn.MaxPool3d(kernel_size=2, stride=2)  # Max Pooling 層 1
        self.dropout1 = nn.Dropout(p=0.2)

        self.layer1 = self._make_layer(64, 64, stride=1)
        self.pool2 = nn.MaxPool3d(kernel_size=2, stride=2)  # Max Pooling 層 2
        self.dropout2 = nn.Dropout(p=0.2)

        self.layer2 = self._make_layer(64, 128, stride=2)
        self.pool3 = nn.MaxPool3d(kernel_size=2, stride=2)
        self.dropout3 = nn.Dropout(p=0.2)
        
        self.layer3 = self._make_layer(128, 256, stride=2)
        # self.pool4 = nn.MaxPool3d(kernel_size=2, stride=2)
        self.dropout4 = nn.Dropout(p=0.2)
        
        self.layer4 = self._make_layer(256, 512, stride=2)
        self.pool5 = nn.MaxPool3d(kernel_size=2, stride=2)
        # self.dropout5 = nn.Dropout(p=0.2)

        self.fc = nn.Linear(512, num_classes)   # 調整為輸出大小

    def _make_layer(self, in_channels, out_channels, stride):
        return nn.Sequential(
            BasicBlock3D(in_channels, out_channels, stride),
            BasicBlock3D(out_channels, out_channels, 1)
        )

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.pool1(out)   # 使用 Max Pooling
        out = self.dropout1(out)

        out = self.layer1(out)
        out = self.pool2(out)   # 使用 Max Pooling
        out = self.dropout2(out)

        out = self.layer2(out)
        out = self.pool3(out)  
        out = self.dropout3(out)

        out = self.layer3(out)
        # out = self.pool4(out)
        out = self.dropout4(out)

        out = self.layer4(out)
        out = self.pool5(out)
        # out = self.dropout5(out)

        out = F.adaptive_avg_pool3d(out, (1, 1, 1))
        out = out.view(out.size(0), -1)
        out = self.fc(out)
        return out

def train_model(train_loader, model, criterion, optimizer, num_epochs, device):
    model.to(device)
    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        for batch in train_loader:
            inputs = batch["vol"].to(device)
            labels = batch["label"].float().to(device)
            labels = labels.view(-1, 1)  # 確保標籤形狀為 (batch_size, 1)

            optimizer.zero_grad()
            outputs = model(inputs)
            
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item()
            
            predicted = (torch.sigmoid(outputs) >= 0.5).float()
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
        
        epoch_loss = running_loss / len(train_loader)
        train_accuracy = correct / total
        
        logging.info(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {train_accuracy:.4f}')

def test_model(test_loader, model, criterion, device):
    model.eval()
    model.to(device)
    total_test_loss = 0.0
    correct_test = 0
    total_test = 0
    
    all_labels = []
    all_predictions = []
    
    with torch.no_grad():
        for batch in test_loader:
            inputs = batch["vol"].to(device)
            labels = batch["label"].float().to(device)
            labels = labels.view(-1, 1)  # 確保標籤形狀為 (batch_size, 1)
            logging.info(f'Test Labels: {labels}, Type: {labels.dtype}, Unique Values: {labels.unique()}')

            outputs = model(inputs)
            logging.info(f'Raw outputs: {outputs}, Type: {outputs.dtype}')
            
            loss = criterion(outputs, labels)
            total_test_loss += loss.item()
            
            predicted = (torch.sigmoid(outputs) >= 0.5).float()
            total_test += labels.size(0)
            correct_test += (predicted == labels).sum().item()
            
            # 收集所有標籤和預測值，後續計算 F1 Score 和混淆矩陣
            all_labels.extend(labels.cpu().numpy())
            all_predictions.extend(predicted.cpu().numpy())

        # 計算 F1 Score 和混淆矩陣
        from sklearn.metrics import f1_score, confusion_matrix
        f1 = f1_score(all_labels, all_predictions)
        cm = confusion_matrix(all_labels, all_predictions)
        
        # 混淆矩陣中的 TN, FP, FN, TP
        if cm.size == 4:
            TN, FP, FN, TP = cm.ravel()
        else:
            # 處理只有一個類別的情況
            TN = FP = FN = TP = 0
            if len(cm) == 1:
                if all_labels[0] == 0:
                    TN = cm[0][0]
                else:
                    TP = cm[0][0]

        average_test_loss = total_test_loss / len(test_loader)
        test_accuracy = correct_test / total_test if total_test > 0 else 0
        
        # 計算靈敏度和特異性
        sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0
        specificity = TN / (TN + FP) if (TN + FP) > 0 else 0
        
        logging.info(f'Average Test Loss: {average_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')
        logging.info(f'Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}')
        logging.info(f'F1 Score: {f1:.4f}')
        logging.info(f'Confusion Matrix:\n{cm}')

if __name__ == "__main__":
    # 資料目錄和標籤文件
    data_directory = "/home/u3861345/data_train_test/first_try_data"
    label_file = "/home/u3861345/data_train_test/first_try_data/labels.csv"
    
    # 準備資料
    train_ds, test_ds = prepare(in_dir=data_directory, label_file=label_file)
    
    # 設定批次大小和工作執行緒數
    batch_size = 1  # 根據您的資源調整
    num_workers = 4  # 根據您的資源調整
    
    # 創建 DataLoader，資料增強將在每次訪問資料時動態應用
    train_loader = DataLoader(
        train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers
    )
    test_loader = DataLoader(
        test_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers
    )
    
    # 初始化模型、損失函數和優化器
    model = ResNet3D(num_classes=1)
    criterion = nn.BCEWithLogitsLoss()  # 損失函數 用於二元分類問題
    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)
    
    # 訓練參數
    num_epochs = 100  # 根據需要調整
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # 訓練模型
    train_model(train_loader, model, criterion, optimizer, num_epochs, device)
    
    # 測試模型
    logging.info("Final Test on External Test Set:")
    test_model(test_loader, model, criterion, device)
